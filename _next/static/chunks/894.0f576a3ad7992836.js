"use strict";(self.webpackChunk_N_E=self.webpackChunk_N_E||[]).push([[894],{3894:function(e,t,n){n.r(t),n.d(t,{frontmatter:function(){return a}});var s=n(7437),r=n(5595);let a={title:"How tf do LLMs work? - Part 1",description:"It all starts with a bit of attention-seeking, I suppose.",date:"2025-09-07",readTime:"12 min read",tags:["transformers","llm","attention","machine-learning"],category:"REVIEW",gradient:"from-blue-500/20 to-purple-500/20"};function i(e){let t=Object.assign({h2:"h2",p:"p",em:"em",strong:"strong",pre:"pre",code:"code",a:"a",blockquote:"blockquote",hr:"hr"},(0,r.ah)(),e.components);return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(t.h2,{children:"(A Megan Fox less) Transformers"}),"\n",(0,s.jsxs)(t.p,{children:["Transformers are a revolutionary set of Neural Network Architectures which changed the course of Machine Learning and Artificial Intelligence; and are the bedrock on which modern LLMs are built. Transformer architectures are designed to handle large amounts of simultaneous data with sequential semantic interpretability. This ability to parse sequential input weighing the semantic importance concurrently relies on a novel mechanism coined ",(0,s.jsx)(t.em,{children:(0,s.jsx)(t.strong,{children:"self-attention"})})," (delved into below) and led to the usage of transformers to interpret and learn the natural languages we write, speak, and hear."]}),"\n",(0,s.jsx)(t.p,{children:"The simultaneous processing of sequential data allows it to model the relationship between any two words (tokens) in the input sequence no matter how far apart they are in the context of a language embedding graph (important calculations for context 'understanding'). The core functionality of the architecture is to transform an input sequence of vectors into an output sequence of vectors, where each output vector is a rich (and in cases complex, due to the semantic ambiguity of natural language) representation of its corresponding input, contextualised in depth by its relationship with all other elements in the sequence. This was considered a huge step forward from previous established architectures which processed inputs and computed sequentially (check out the note below for previous limitations)."}),"\n",(0,s.jsx)(t.h2,{children:"Limitations of Previous SOTA Model - RNNs with LSTM"}),"\n",(0,s.jsx)(t.p,{children:"The state-of-the-art model prior to the introduction of the Transformer was the Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) /+ Gated Recurrent Unit (GRU). Their limitations were not just a byproduct but were fundamental to their design:"}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Sequential Bottleneck:"})," RNNs process data one element at a time. The hidden state for timestep ",(0,s.jsx)(t.em,{children:"t"}),", denoted ",(0,s.jsxs)(t.em,{children:["h",(0,s.jsx)("sub",{children:"t"})]}),", is a function of the input ",(0,s.jsxs)(t.em,{children:["x",(0,s.jsx)("sub",{children:"t"})]})," and the previous hidden state ",(0,s.jsxs)(t.em,{children:["h",(0,s.jsx)("sub",{children:"tâˆ’1"})]}),". This inherent sequentiality means you cannot compute the state for the 10th word until you have computed it for the 9th."]}),"\n",(0,s.jsx)(t.pre,{children:(0,s.jsx)(t.code,{children:"h_t = f(x_t, h_{t-1})\n"})}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Long Range Dependencies:"})," Initial information of a sequence has to pass through a long chain of computations to influence elements at the end. Even with the gating mechanisms of LSTMs (which were designed to combat this), the gradient signal could become too weak (vanish) or too strong (explode) when back-propagated over many timesteps. (",(0,s.jsx)(t.em,{children:"Note: For a Transformer, the path between any two tokens is a direct computation within the attention mechanism, making the distance effectively 1 for any relationship."}),")"]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Leaving Chips on the Table:"})," Modern GPUs/TPUs are built to be parallel processing behemoths (1000s of operations at once) and RNN's intrinsic sequentiality is literally leaving all computation potential on the table. This made training on web-scale datasets incredibly slow and impractical."]}),"\n",(0,s.jsx)(t.h2,{children:"Why 'Traditional' Attention Wasn't Enough"}),"\n",(0,s.jsx)(t.p,{children:"Before the release of the paper, attention mechanisms existed, but they were almost always used in conjunction with Recurrent Neural Networks (RNNs). For instance, in machine translation, an RNN encoder processed the source sentence, while an attention mechanism helped the RNN decoder focus on relevant parts at each translation step."}),"\n",(0,s.jsx)(t.p,{children:"However, the limitation remained the RNN backbone. The entire process was still constrained by the sequential bottleneck of the RNN. Attention was an enhancement, not the core engine."}),"\n",(0,s.jsxs)(t.p,{children:["The groundbreaking research paper ",(0,s.jsx)(t.a,{href:"https://arxiv.org/pdf/1706.03762",children:(0,s.jsx)(t.strong,{children:'"Attention Is All You Need" by Vaswani et al., 2017'})})," [1] posed a radical question: what if we discarded the recurrence entirely and built a model architecture solely based on attention?"]}),"\n",(0,s.jsx)(t.p,{children:"The research demonstrated that self-attention was not just an add-on; it was powerful enough on its own to handle complex sequence modelling tasks. By removing the dependency on a recurring (and sequential) architecture, they unlocked the potential for massive parallelisation."}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsx)(t.h2,{children:"Parallel Processing is crucial for scaling up any architecture, btw."}),"\n"]}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Hardware Utilisation:"})," Modern processors (GPUs (Graphics)/TPUs (Tensors)) are parallel processors with thousands of cores. Model architectures that can be expressed as a series of large, independent calculations (matrix multiplications) will run exponentially faster than any sequential model ever could."]}),"\n"]}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Scaling Law:"})," (Predictable relationship shown by a lot of research) The bigger the model (more parameters) and the more data you train it on, the better it performs. This was the driving force behind the first few generations of LLMs. ",(0,s.jsxs)(t.em,{children:["(Note: this is a huge point of contention lately as it seems that the scaling graph has plateaued somewhat i.e. LLM providers have seemed to hit a wall in terms of scaling up parameters, compute, and training data. For more info, please check out this exceptional ",(0,s.jsx)(t.a,{href:"https://cameronrwolfe.substack.com/p/llm-scaling-laws",children:"blog piece by Dr, Cameron R. Wolfe"})," [2] about how scaling laws are applicable to LLMs and exploring other paths to drive AI progress forward.)"]})]}),"\n"]}),"\n",(0,s.jsxs)(t.blockquote,{children:["\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"Feasibility:"})," Parallel processing is what makes this scaling law possible in LLMs. Without it, training a 100+ billion parameter model would take years or decades, making it practically impossible. The Transformer's parallel-friendly design is the single biggest reason we have been able to witness the scaling of models to their current size (and capability)."]}),"\n"]}),"\n",(0,s.jsx)(t.h2,{children:"A Brief Overview of the Proposed Transformer Structure (from [1])"}),"\n",(0,s.jsxs)(t.p,{children:["The original Transformer from the paper featured an ",(0,s.jsx)(t.strong,{children:"Encoder-Decoder"})," architecture, primarily for machine translation."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The Encoder:"}),' Composed of a stack of identical layers. Its job is to read the entire input sequence (e.g., "The cat sat on the mat") and produce a context-rich numerical representation (referred to as embeddings) for each token. Each encoder layer has two main sub-layers: a ',(0,s.jsx)(t.strong,{children:"Multi-Head Self-Attention"})," mechanism and a ",(0,s.jsx)(t.strong,{children:"Position-wise Feed-Forward Network"})," (delved into in the next parts of the series)."]}),"\n",(0,s.jsxs)(t.p,{children:[(0,s.jsx)(t.strong,{children:"The Decoder:"})," Also a stack of layers. Its job is to take the encoder's output and generate the target sequence token-by-token (e.g., \"Le chat s'est assis sur le tapis\"). Each decoder layer has ",(0,s.jsx)(t.em,{children:"three"})," sub-layers: a ",(0,s.jsx)(t.strong,{children:"Masked Multi-Head Self-Attention"})," (masked to 'blind' it from seeing future tokens it's supposed to predict), an ",(0,s.jsx)(t.strong,{children:"Encoder-Decoder Attention"})," (to look at the output from the encoder), and a ",(0,s.jsx)(t.strong,{children:"Feed-Forward Network"})," (delved into in the next parts of the series)."]}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsxs)(t.em,{children:[(0,s.jsx)(t.strong,{children:"Note"}),": While this was the proposed architecture in 2017, modern LLMs like GPT, BERT etc use ",(0,s.jsx)(t.strong,{children:"Decoder-only"})," architectures."]})}),"\n",(0,s.jsx)(t.hr,{}),"\n",(0,s.jsx)(t.p,{children:(0,s.jsx)(t.em,{children:"[This blog is part 1 of 6 in the series 'How tf do LLMs work?'. More parts will (hopefully) be released on a weekly basis (Reason: Attention Spans are bad, now). Please let me know what you thought of this piece and what you would like to see next! Much love!]"})}),"\n",(0,s.jsx)(t.h2,{children:"References"}),"\n",(0,s.jsx)(t.p,{children:"[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30."}),"\n",(0,s.jsx)(t.p,{children:"[2] Wolfe, C. R. (2024). LLM Scaling Laws. Retrieved from https://cameronrwolfe.substack.com/p/llm-scaling-laws"})]})}t.default=function(){let e=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},{wrapper:t}=Object.assign({},(0,r.ah)(),e.components);return t?(0,s.jsx)(t,Object.assign({},e,{children:(0,s.jsx)(i,e)})):i(e)}}}]);