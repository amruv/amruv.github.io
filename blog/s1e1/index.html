<!DOCTYPE html><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/e60f9d475e84d1a8.css" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-41a9ee0a1d65a3d2.js"/><script src="/_next/static/chunks/fd9d1056-142b32a47bdeb1fe.js" async=""></script><script src="/_next/static/chunks/117-592c072c4cfcd412.js" async=""></script><script src="/_next/static/chunks/main-app-00fa78824fd439d4.js" async=""></script><script src="/_next/static/chunks/972-0ab9a2270933faa5.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-ec9f36fd061ae673.js" async=""></script><title>Sai Amruth Balusu</title><meta name="description" content="Personal website of Sai Amruth Balusu, Machine Learning Researcher specializing in transformers, computer vision, and distributed systems."/><meta name="author" content="Sai Amruth Balusu"/><meta name="keywords" content="machine learning,AI research,deep learning,transformers,computer vision"/><meta property="og:title" content="Sai Amruth Balusu - ML Researcher"/><meta property="og:description" content="Personal website showcasing research, publications, and projects in machine learning."/><meta property="og:url" content="https://amruv.github.io/"/><meta property="og:site_name" content="Sai Amruth Balusu"/><meta property="og:image" content="https://amruv.github.io/og-image.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Sai Amruth Balusu - ML Researcher"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Sai Amruth Balusu - ML Researcher"/><meta name="twitter:description" content="Personal website showcasing research, publications, and projects in machine learning."/><meta name="twitter:image" content="https://amruv.github.io/og-image.png"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-42372ed130431b0a.js" noModule=""></script></head><body class="__className_f367f3"><div class="min-h-screen bg-background text-foreground"><main class="min-h-screen bg-background text-foreground"><div class="bg-background border-b border-border"><div class="max-w-4xl mx-auto px-6 py-8"><div class="flex items-center gap-4 mb-6"><a href="/blog/"><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4 mr-2"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg>Back to Blog</button></a></div></div></div><div class="max-w-4xl mx-auto px-6 py-12"><div><div class="flex items-center gap-2 mb-4"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-foreground text-xs font-medium">REVIEW</div><div class="flex items-center space-x-4 text-sm text-muted-foreground"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>2025-09-07</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>12 min read</span></div></div></div><h1 class="text-4xl md:text-5xl font-bold mb-6 test-font-courier text-primary leading-tight">How tf do LLMs work? - Part 1</h1><p class="text-xl text-muted-foreground mb-8 test-font-mono leading-relaxed">It all starts with a bit of attention-seeking, I suppose.</p><div class="flex flex-wrap gap-2 mb-8"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">transformers</div><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">llm</div><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">attention</div><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">machine-learning</div></div><div class="flex items-center gap-4 pb-8 border-b border-border"><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4 mr-2"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg>Share</button><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4 mr-2"><path d="M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z"></path><path d="M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z"></path></svg>Save</button></div></div></div><div class="max-w-4xl mx-auto px-6 pb-20"><div class="relative rounded-xl bg-card text-card-foreground border-0 shadow-none"><div class="p-0"><div class="prose prose-lg max-w-none test-font-mono" style="color:var(--foreground);line-height:1.7">## (A Megan Fox less) Transformers


Transformers are a revolutionary set of Neural Network Architectures which changed the course of Machine Learning and Artificial Intelligence; and are the bedrock on which modern LLMs are built. Transformer architectures are designed to handle large amounts of simultaneous data with sequential semantic interpretability. This ability to parse sequential input weighing the semantic importance concurrently relies on a novel mechanism coined ***self-attention*** (delved into below) and led to the usage of transformers to interpret and learn the natural languages we write, speak, and hear.

The simultaneous processing of sequential data allows it to model the relationship between any two words (tokens) in the input sequence no matter how far apart they are in the context of a language embedding graph (important calculations for context &#x27;understanding&#x27;). The core functionality of the architecture is to transform an input sequence of vectors into an output sequence of vectors, where each output vector is a rich (and in cases complex, due to the semantic ambiguity of natural language) representation of its corresponding input, contextualised in depth by its relationship with all other elements in the sequence. This was considered a huge step forward from previous established architectures which processed inputs and computed sequentially (check out the note below for previous limitations).


## Limitations of Previous SOTA Model - RNNs with LSTM

The state-of-the-art model prior to the introduction of the Transformer was the Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) /+ Gated Recurrent Unit (GRU). Their limitations were not just a byproduct but were fundamental to their design:

**Sequential Bottleneck:** RNNs process data one element at a time. The hidden state for timestep *t*, denoted *h&lt;sub&gt;t&lt;/sub&gt;*, is a function of the input *x&lt;sub&gt;t&lt;/sub&gt;* and the previous hidden state *h&lt;sub&gt;t−1&lt;/sub&gt;*. This inherent sequentiality means you cannot compute the state for the 10th word until you have computed it for the 9th.

```
h_t = f(x_t, h_{t-1})
```

**Long Range Dependencies:** Initial information of a sequence has to pass through a long chain of computations to influence elements at the end. Even with the gating mechanisms of LSTMs (which were designed to combat this), the gradient signal could become too weak (vanish) or too strong (explode) when back-propagated over many timesteps. (*Note: For a Transformer, the path between any two tokens is a direct computation within the attention mechanism, making the distance effectively 1 for any relationship.*)

**Leaving Chips on the Table:** Modern GPUs/TPUs are built to be parallel processing behemoths (1000s of operations at once) and RNN&#x27;s intrinsic sequentiality is literally leaving all computation potential on the table. This made training on web-scale datasets incredibly slow and impractical.

## Why &#x27;Traditional&#x27; Attention Wasn&#x27;t Enough

Before the release of the paper, attention mechanisms existed, but they were almost always used in conjunction with Recurrent Neural Networks (RNNs). For instance, in machine translation, an RNN encoder processed the source sentence, while an attention mechanism helped the RNN decoder focus on relevant parts at each translation step.

However, the limitation remained the RNN backbone. The entire process was still constrained by the sequential bottleneck of the RNN. Attention was an enhancement, not the core engine.

The groundbreaking research paper [**&quot;Attention Is All You Need&quot; by Vaswani et al., 2017**](https://arxiv.org/pdf/1706.03762) [1] posed a radical question: what if we discarded the recurrence entirely and built a model architecture solely based on attention?

The research demonstrated that self-attention was not just an add-on; it was powerful enough on its own to handle complex sequence modelling tasks. By removing the dependency on a recurring (and sequential) architecture, they unlocked the potential for massive parallelisation.

## Parallel Processing is crucial for scaling up any architecture, btw.

**Hardware Utilisation:** Modern processors (GPUs (Graphics)/TPUs (Tensors)) are parallel processors with thousands of cores. Model architectures that can be expressed as a series of large, independent calculations (matrix multiplications) will run exponentially faster than any sequential model ever could.

**Scaling Law:** (Predictable relationship shown by a lot of research) The bigger the model (more parameters) and the more data you train it on, the better it performs. This was the driving force behind the first few generations of LLMs. *(Note: this is a huge point of contention lately as it seems that the scaling graph has plateaued somewhat i.e. LLM providers have seemed to hit a wall in terms of scaling up parameters, compute, and training data. For more info, please check out this exceptional [blog piece by Dr, Cameron R. Wolfe](https://cameronrwolfe.substack.com/p/llm-scaling-laws) [2] about how scaling laws are applicable to LLMs and exploring other paths to drive AI progress forward.)*

**Feasibility:** Parallel processing is what makes this scaling law possible in LLMs. Without it, training a 100+ billion parameter model would take years or decades, making it practically impossible. The Transformer&#x27;s parallel-friendly design is the single biggest reason we have been able to witness the scaling of models to their current size (and capability).

## A Brief Overview of the Proposed Transformer Structure (from [1])

The original Transformer from the paper featured an **Encoder-Decoder** architecture, primarily for machine translation.

**The Encoder:** Composed of a stack of identical layers. Its job is to read the entire input sequence (e.g., &quot;The cat sat on the mat&quot;) and produce a context-rich numerical representation (referred to as embeddings) for each token. Each encoder layer has two main sub-layers: a **Multi-Head Self-Attention** mechanism and a **Position-wise Feed-Forward Network** (delved into in the next parts of the series).

**The Decoder:** Also a stack of layers. Its job is to take the encoder&#x27;s output and generate the target sequence token-by-token (e.g., &quot;Le chat s&#x27;est assis sur le tapis&quot;). Each decoder layer has *three* sub-layers: a **Masked Multi-Head Self-Attention** (masked to &#x27;blind&#x27; it from seeing future tokens it&#x27;s supposed to predict), an **Encoder-Decoder Attention** (to look at the output from the encoder), and a **Feed-Forward Network** (delved into in the next parts of the series).

***Note**: While this was the proposed architecture in 2017, modern LLMs like GPT, BERT etc use **Decoder-only** architectures.*

---

*[This blog is part 1 of 6 in the series &#x27;How tf do LLMs work?&#x27;. More parts will (hopefully) be released on a weekly basis (Reason: Attention Spans are bad, now). Please let me know what you thought of this piece and what you would like to see next! Much love!]*

## References

[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

[2] Wolfe, C. R. (2024). LLM Scaling Laws. Retrieved from https://cameronrwolfe.substack.com/p/llm-scaling-laws</div></div></div></div></main></div><script src="/_next/static/chunks/webpack-41a9ee0a1d65a3d2.js" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/e60f9d475e84d1a8.css\",\"style\"]\n"])</script><script>self.__next_f.push([1,"3:I[2846,[],\"\"]\n6:I[4707,[],\"\"]\n8:I[6423,[],\"\"]\na:I[1060,[],\"\"]\n7:[\"slug\",\"s1e1\",\"d\"]\nb:[]\n"])</script><script>self.__next_f.push([1,"0:[\"$\",\"$L3\",null,{\"buildId\":\"yr3HlNOzvrtHfX5ZM7Zr_\",\"assetPrefix\":\"\",\"urlParts\":[\"\",\"blog\",\"s1e1\",\"\"],\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"s1e1\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"s1e1\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialSeedData\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"s1e1\",\"d\"],{\"children\":[\"__PAGE__\",{},[[\"$L4\",\"$L5\",null],null],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",\"$7\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[null,[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\"}]],null]},[[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/e60f9d475e84d1a8.css\",\"precedence\":\"next\",\"crossOrigin\":\"$undefined\"}]],[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_f367f3\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background text-foreground\",\"children\":[\"$\",\"$L6\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[]}]}]}]}]],null],null],\"couldBeIntercepted\":false,\"initialHead\":[null,\"$L9\"],\"globalErrorComponent\":\"$a\",\"missingSlots\":\"$Wb\"}]\n"])</script><script>self.__next_f.push([1,"c:I[2972,[\"972\",\"static/chunks/972-0ab9a2270933faa5.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-ec9f36fd061ae673.js\"],\"\"]\nd:I[3263,[\"972\",\"static/chunks/972-0ab9a2270933faa5.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-ec9f36fd061ae673.js\"],\"default\"]\ne:T1c8d,"])</script><script>self.__next_f.push([1,"## (A Megan Fox less) Transformers\n\n\nTransformers are a revolutionary set of Neural Network Architectures which changed the course of Machine Learning and Artificial Intelligence; and are the bedrock on which modern LLMs are built. Transformer architectures are designed to handle large amounts of simultaneous data with sequential semantic interpretability. This ability to parse sequential input weighing the semantic importance concurrently relies on a novel mechanism coined ***self-attention*** (delved into below) and led to the usage of transformers to interpret and learn the natural languages we write, speak, and hear.\n\nThe simultaneous processing of sequential data allows it to model the relationship between any two words (tokens) in the input sequence no matter how far apart they are in the context of a language embedding graph (important calculations for context 'understanding'). The core functionality of the architecture is to transform an input sequence of vectors into an output sequence of vectors, where each output vector is a rich (and in cases complex, due to the semantic ambiguity of natural language) representation of its corresponding input, contextualised in depth by its relationship with all other elements in the sequence. This was considered a huge step forward from previous established architectures which processed inputs and computed sequentially (check out the note below for previous limitations).\n\n\n## Limitations of Previous SOTA Model - RNNs with LSTM\n\nThe state-of-the-art model prior to the introduction of the Transformer was the Recurrent Neural Network (RNN) with Long Short Term Memory (LSTM) /+ Gated Recurrent Unit (GRU). Their limitations were not just a byproduct but were fundamental to their design:\n\n**Sequential Bottleneck:** RNNs process data one element at a time. The hidden state for timestep *t*, denoted *h\u003csub\u003et\u003c/sub\u003e*, is a function of the input *x\u003csub\u003et\u003c/sub\u003e* and the previous hidden state *h\u003csub\u003et−1\u003c/sub\u003e*. This inherent sequentiality means you cannot compute the state for the 10th word until you have computed it for the 9th.\n\n```\nh_t = f(x_t, h_{t-1})\n```\n\n**Long Range Dependencies:** Initial information of a sequence has to pass through a long chain of computations to influence elements at the end. Even with the gating mechanisms of LSTMs (which were designed to combat this), the gradient signal could become too weak (vanish) or too strong (explode) when back-propagated over many timesteps. (*Note: For a Transformer, the path between any two tokens is a direct computation within the attention mechanism, making the distance effectively 1 for any relationship.*)\n\n**Leaving Chips on the Table:** Modern GPUs/TPUs are built to be parallel processing behemoths (1000s of operations at once) and RNN's intrinsic sequentiality is literally leaving all computation potential on the table. This made training on web-scale datasets incredibly slow and impractical.\n\n## Why 'Traditional' Attention Wasn't Enough\n\nBefore the release of the paper, attention mechanisms existed, but they were almost always used in conjunction with Recurrent Neural Networks (RNNs). For instance, in machine translation, an RNN encoder processed the source sentence, while an attention mechanism helped the RNN decoder focus on relevant parts at each translation step.\n\nHowever, the limitation remained the RNN backbone. The entire process was still constrained by the sequential bottleneck of the RNN. Attention was an enhancement, not the core engine.\n\nThe groundbreaking research paper [**\"Attention Is All You Need\" by Vaswani et al., 2017**](https://arxiv.org/pdf/1706.03762) [1] posed a radical question: what if we discarded the recurrence entirely and built a model architecture solely based on attention?\n\nThe research demonstrated that self-attention was not just an add-on; it was powerful enough on its own to handle complex sequence modelling tasks. By removing the dependency on a recurring (and sequential) architecture, they unlocked the potential for massive parallelisation.\n\n## Parallel Processing is crucial for scaling up any architecture, btw.\n\n**Hardware Utilisation:** Modern processors (GPUs (Graphics)/TPUs (Tensors)) are parallel processors with thousands of cores. Model architectures that can be expressed as a series of large, independent calculations (matrix multiplications) will run exponentially faster than any sequential model ever could.\n\n**Scaling Law:** (Predictable relationship shown by a lot of research) The bigger the model (more parameters) and the more data you train it on, the better it performs. This was the driving force behind the first few generations of LLMs. *(Note: this is a huge point of contention lately as it seems that the scaling graph has plateaued somewhat i.e. LLM providers have seemed to hit a wall in terms of scaling up parameters, compute, and training data. For more info, please check out this exceptional [blog piece by Dr, Cameron R. Wolfe](https://cameronrwolfe.substack.com/p/llm-scaling-laws) [2] about how scaling laws are applicable to LLMs and exploring other paths to drive AI progress forward.)*\n\n**Feasibility:** Parallel processing is what makes this scaling law possible in LLMs. Without it, training a 100+ billion parameter model would take years or decades, making it practically impossible. The Transformer's parallel-friendly design is the single biggest reason we have been able to witness the scaling of models to their current size (and capability).\n\n## A Brief Overview of the Proposed Transformer Structure (from [1])\n\nThe original Transformer from the paper featured an **Encoder-Decoder** architecture, primarily for machine translation.\n\n**The Encoder:** Composed of a stack of identical layers. Its job is to read the entire input sequence (e.g., \"The cat sat on the mat\") and produce a context-rich numerical representation (referred to as embeddings) for each token. Each encoder layer has two main sub-layers: a **Multi-Head Self-Attention** mechanism and a **Position-wise Feed-Forward Network** (delved into in the next parts of the series).\n\n**The Decoder:** Also a stack of layers. Its job is to take the encoder's output and generate the target sequence token-by-token (e.g., \"Le chat s'est assis sur le tapis\"). Each decoder layer has *three* sub-layers: a **Masked Multi-Head Self-Attention** (masked to 'blind' it from seeing future tokens it's supposed to predict), an **Encoder-Decoder Attention** (to look at the output from the encoder), and a **Feed-Forward Network** (delved into in the next parts of the series).\n\n***Note**: While this was the proposed architecture in 2017, modern LLMs like GPT, BERT etc use **Decoder-only** architectures.*\n\n---\n\n*[This blog is part 1 of 6 in the series 'How tf do LLMs work?'. More parts will (hopefully) be released on a weekly basis (Reason: Attention Spans are bad, now). Please let me know what you thought of this piece and what you would like to see next! Much love!]*\n\n## References\n\n[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... \u0026 Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.\n\n[2] Wolfe, C. R. (2024). LLM Scaling Laws. Retrieved from https://cameronrwolfe.substack.com/p/llm-scaling-laws"])</script><script>self.__next_f.push([1,"5:[\"$\",\"main\",null,{\"className\":\"min-h-screen bg-background text-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-background border-b border-border\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 mb-6\",\"children\":[\"$\",\"$Lc\",null,{\"href\":\"/blog\",\"children\":[\"$\",\"button\",null,{\"className\":\"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4 mr-2\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],\"Back to Blog\"]}]}]}]}]}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-12\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-2 mb-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-foreground text-xs font-medium\",\"children\":\"REVIEW\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4\",\"children\":[[\"$\",\"rect\",\"eu3xkr\",{\"width\":\"18\",\"height\":\"18\",\"x\":\"3\",\"y\":\"4\",\"rx\":\"2\",\"ry\":\"2\"}],[\"$\",\"line\",\"m3sa8f\",{\"x1\":\"16\",\"x2\":\"16\",\"y1\":\"2\",\"y2\":\"6\"}],[\"$\",\"line\",\"18kwsl\",{\"x1\":\"8\",\"x2\":\"8\",\"y1\":\"2\",\"y2\":\"6\"}],[\"$\",\"line\",\"xt86sb\",{\"x1\":\"3\",\"x2\":\"21\",\"y1\":\"10\",\"y2\":\"10\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"2025-09-07\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4\",\"children\":[[\"$\",\"circle\",\"1mglay\",{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"polyline\",\"68esgv\",{\"points\":\"12 6 12 12 16 14\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"12 min read\"}]]}]]}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-bold mb-6 test-font-courier text-primary leading-tight\",\"children\":\"How tf do LLMs work? - Part 1\"}],[\"$\",\"p\",null,{\"className\":\"text-xl text-muted-foreground mb-8 test-font-mono leading-relaxed\",\"children\":\"It all starts with a bit of attention-seeking, I suppose.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"transformers\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"llm\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"attention\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"machine-learning\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 pb-8 border-b border-border\",\"children\":[[\"$\",\"button\",null,{\"className\":\"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4 mr-2\",\"children\":[[\"$\",\"circle\",\"gq8acd\",{\"cx\":\"18\",\"cy\":\"5\",\"r\":\"3\"}],[\"$\",\"circle\",\"w7nqdw\",{\"cx\":\"6\",\"cy\":\"12\",\"r\":\"3\"}],[\"$\",\"circle\",\"1xt0gg\",{\"cx\":\"18\",\"cy\":\"19\",\"r\":\"3\"}],[\"$\",\"line\",\"47mynk\",{\"x1\":\"8.59\",\"x2\":\"15.42\",\"y1\":\"13.51\",\"y2\":\"17.49\"}],[\"$\",\"line\",\"1n3mei\",{\"x1\":\"15.41\",\"x2\":\"8.59\",\"y1\":\"6.51\",\"y2\":\"10.49\"}],\"$undefined\"]}],\"Share\"]}],[\"$\",\"button\",null,{\"className\":\"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4 mr-2\",\"children\":[[\"$\",\"path\",\"vv98re\",{\"d\":\"M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z\"}],[\"$\",\"path\",\"1cyq3y\",{\"d\":\"M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z\"}],\"$undefined\"]}],\"Save\"]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 pb-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative rounded-xl bg-card text-card-foreground border-0 shadow-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"p-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none test-font-mono\",\"style\":{\"color\":\"var(--foreground)\",\"lineHeight\":\"1.7\"},\"children\":[\"$\",\"$Ld\",null,{\"content\":\"$e\",\"frontmatter\":{\"title\":\"How tf do LLMs work? - Part 1\",\"description\":\"It all starts with a bit of attention-seeking, I suppose.\",\"date\":\"2025-09-07\",\"readTime\":\"12 min read\",\"tags\":[\"transformers\",\"llm\",\"attention\",\"machine-learning\"],\"category\":\"REVIEW\",\"gradient\":\"from-blue-500/20 to-purple-500/20\"}}]}]}]}]}]]}]\n"])</script><script>self.__next_f.push([1,"9:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Sai Amruth Balusu\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Personal website of Sai Amruth Balusu, Machine Learning Researcher specializing in transformers, computer vision, and distributed systems.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"Sai Amruth Balusu\"}],[\"$\",\"meta\",\"5\",{\"name\":\"keywords\",\"content\":\"machine learning,AI research,deep learning,transformers,computer vision\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Sai Amruth Balusu - ML Researcher\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Personal website showcasing research, publications, and projects in machine learning.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://amruv.github.io/\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"Sai Amruth Balusu\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:image\",\"content\":\"https://amruv.github.io/og-image.png\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image:alt\",\"content\":\"Sai Amruth Balusu - ML Researcher\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:title\",\"content\":\"Sai Amruth Balusu - ML Researcher\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:description\",\"content\":\"Personal website showcasing research, publications, and projects in machine learning.\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:image\",\"content\":\"https://amruv.github.io/og-image.png\"}],[\"$\",\"meta\",\"19\",{\"name\":\"next-size-adjust\"}]]\n4:null\n"])</script></body></html>