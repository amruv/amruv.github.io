<!DOCTYPE html><html lang="en" class="dark"><head><meta charSet="utf-8"/><meta name="viewport" content="width=device-width, initial-scale=1"/><link rel="preload" href="/_next/static/media/e4af272ccee01ff0-s.p.woff2" as="font" crossorigin="" type="font/woff2"/><link rel="stylesheet" href="/_next/static/css/5ce533f03faec87d.css" crossorigin="" data-precedence="next"/><link rel="preload" as="script" fetchPriority="low" href="/_next/static/chunks/webpack-52aa0ba6fea5041f.js" crossorigin=""/><script src="/_next/static/chunks/fd9d1056-234bd86a4781b182.js" async="" crossorigin=""></script><script src="/_next/static/chunks/472-b42ba595826939f4.js" async="" crossorigin=""></script><script src="/_next/static/chunks/main-app-a0b391702181ffac.js" async="" crossorigin=""></script><script src="/_next/static/chunks/326-f3014610e927a105.js" async=""></script><script src="/_next/static/chunks/app/blog/%5Bslug%5D/page-59ec5e1211dbfc7e.js" async=""></script><title>Sai Amruth Balusu</title><meta name="description" content="Personal website of Sai Amruth Balusu, Machine Learning Researcher specializing in transformers, computer vision, and distributed systems."/><meta name="author" content="Sai Amruth Balusu"/><meta name="keywords" content="machine learning,AI research,deep learning,transformers,computer vision"/><meta property="og:title" content="Sai Amruth Balusu - ML Researcher"/><meta property="og:description" content="Personal website showcasing research, publications, and projects in machine learning."/><meta property="og:url" content="https://amruv.github.io/"/><meta property="og:site_name" content="Sai Amruth Balusu"/><meta property="og:image" content="https://amruv.github.io/og-image.png"/><meta property="og:image:width" content="1200"/><meta property="og:image:height" content="630"/><meta property="og:image:alt" content="Sai Amruth Balusu - ML Researcher"/><meta property="og:type" content="website"/><meta name="twitter:card" content="summary_large_image"/><meta name="twitter:title" content="Sai Amruth Balusu - ML Researcher"/><meta name="twitter:description" content="Personal website showcasing research, publications, and projects in machine learning."/><meta name="twitter:image" content="https://amruv.github.io/og-image.png"/><meta name="next-size-adjust"/><script src="/_next/static/chunks/polyfills-c67a75d1b6f99dc8.js" crossorigin="" noModule=""></script></head><body class="__className_e8ce0c"><div class="min-h-screen bg-background text-foreground"><main class="min-h-screen bg-background text-foreground"><div class="bg-background border-b border-border"><div class="max-w-4xl mx-auto px-6 py-8"><div class="flex items-center gap-4 mb-6"><a href="/blog/"><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4 mr-2"><path d="m12 19-7-7 7-7"></path><path d="M19 12H5"></path></svg>Back to Blog</button></a></div></div></div><div class="max-w-4xl mx-auto px-6 py-12"><div><div class="flex items-center gap-2 mb-4"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-foreground text-xs font-medium">TUTORIAL</div><div class="flex items-center space-x-4 text-sm text-muted-foreground"><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><rect width="18" height="18" x="3" y="4" rx="2" ry="2"></rect><line x1="16" x2="16" y1="2" y2="6"></line><line x1="8" x2="8" y1="2" y2="6"></line><line x1="3" x2="21" y1="10" y2="10"></line></svg><span>March 15, 2024</span></div><div class="flex items-center space-x-1"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4"><circle cx="12" cy="12" r="10"></circle><polyline points="12 6 12 12 16 14"></polyline></svg><span>12 min read</span></div></div></div><h1 class="text-4xl md:text-5xl font-bold mb-6 test-font-courier text-primary leading-tight">Building Efficient Transformers from Scratch</h1><p class="text-xl text-muted-foreground mb-8 test-font-mono leading-relaxed">A comprehensive guide to implementing transformer architectures with optimizations for memory and compute efficiency. Includes PyTorch code examples and performance benchmarks.</p><div class="flex flex-wrap gap-2 mb-8"><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">PyTorch</div><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">Transformers</div><div class="inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm">Tutorial</div></div><div class="flex items-center gap-4 pb-8 border-b border-border"><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4 mr-2"><circle cx="18" cy="5" r="3"></circle><circle cx="6" cy="12" r="3"></circle><circle cx="18" cy="19" r="3"></circle><line x1="8.59" x2="15.42" y1="13.51" y2="17.49"></line><line x1="15.41" x2="8.59" y1="6.51" y2="10.49"></line></svg>Share</button><button class="inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="w-4 h-4 mr-2"><path d="M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z"></path><path d="M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z"></path></svg>Save</button></div></div></div><div class="max-w-4xl mx-auto px-6 pb-20"><div class="relative rounded-xl bg-card text-card-foreground border-0 shadow-none"><div class="p-0"><div class="prose prose-lg max-w-none test-font-mono" style="color:var(--foreground);line-height:1.7"><br/><h1 class="text-3xl font-bold mb-6 mt-8 text-primary test-font-courier">Building Efficient Transformers from Scratch</h1><br/><p class="mb-4 leading-relaxed">Transformers have revolutionized the field of natural language processing and beyond. In this comprehensive tutorial, we&#x27;ll build efficient transformer architectures from scratch, focusing on memory and compute optimizations.</p><br/><h2 class="text-2xl font-bold mb-4 mt-6 text-primary test-font-courier">Introduction</h2><br/><p class="mb-4 leading-relaxed">The transformer architecture, introduced in &quot;Attention Is All You Need&quot; (Vaswani et al., 2017), has become the foundation for most state-of-the-art models in NLP, computer vision, and other domains. Understanding how to implement and optimize transformers is crucial for any ML practitioner.</p><br/><h2 class="text-2xl font-bold mb-4 mt-6 text-primary test-font-courier">Key Components</h2><br/><h3 class="text-xl font-semibold mb-3 mt-4 text-primary test-font-courier">1. Multi-Head Attention</h3><br/><p class="mb-4 leading-relaxed">The core innovation of transformers is the multi-head attention mechanism. Here&#x27;s a simplified implementation:</p><br/><p class="mb-4 leading-relaxed">import torch</p><p class="mb-4 leading-relaxed">import torch.nn as nn</p><p class="mb-4 leading-relaxed">import torch.nn.functional as F</p><br/><p class="mb-4 leading-relaxed">class MultiHeadAttention(nn.Module):</p><p class="mb-4 leading-relaxed">    def __init__(self, d_model, num_heads):</p><p class="mb-4 leading-relaxed">        super().__init__()</p><p class="mb-4 leading-relaxed">        self.d_model = d_model</p><p class="mb-4 leading-relaxed">        self.num_heads = num_heads</p><p class="mb-4 leading-relaxed">        self.d_k = d_model // num_heads</p><br/><p class="mb-4 leading-relaxed">        self.W_q = nn.Linear(d_model, d_model)</p><p class="mb-4 leading-relaxed">        self.W_k = nn.Linear(d_model, d_model)</p><p class="mb-4 leading-relaxed">        self.W_v = nn.Linear(d_model, d_model)</p><p class="mb-4 leading-relaxed">        self.W_o = nn.Linear(d_model, d_model)</p><br/><p class="mb-4 leading-relaxed">    def forward(self, query, key, value, mask=None):</p><p class="mb-4 leading-relaxed">        batch_size = query.size(0)</p><br/><p class="mb-4 leading-relaxed">        # Linear transformations</p><p class="mb-4 leading-relaxed">        Q = self.W_q(query)</p><p class="mb-4 leading-relaxed">        K = self.W_k(key)</p><p class="mb-4 leading-relaxed">        V = self.W_v(value)</p><br/><p class="mb-4 leading-relaxed">        # Reshape for multi-head attention</p><p class="mb-4 leading-relaxed">        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p><p class="mb-4 leading-relaxed">        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p><p class="mb-4 leading-relaxed">        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)</p><br/><p class="mb-4 leading-relaxed">        # Scaled dot-product attention</p><p class="mb-4 leading-relaxed">        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)</p><br/><p class="mb-4 leading-relaxed">        if mask is not None:</p><p class="mb-4 leading-relaxed">            scores = scores.masked_fill(mask == 0, -1e9)</p><br/><p class="mb-4 leading-relaxed">        attention_weights = F.softmax(scores, dim=-1)</p><p class="mb-4 leading-relaxed">        context = torch.matmul(attention_weights, V)</p><br/><p class="mb-4 leading-relaxed">        # Concatenate heads</p><p class="mb-4 leading-relaxed">        context = context.transpose(1, 2).contiguous().view(</p><p class="mb-4 leading-relaxed">            batch_size, -1, self.d_model</p><p class="mb-4 leading-relaxed">        )</p><br/><p class="mb-4 leading-relaxed">        return self.W_o(context)</p><br/><h3 class="text-xl font-semibold mb-3 mt-4 text-primary test-font-courier">2. Position Encoding</h3><br/><p class="mb-4 leading-relaxed">Since transformers don&#x27;t have inherent positional information, we need to add position encodings:</p><br/><p class="mb-4 leading-relaxed">class PositionalEncoding(nn.Module):</p><p class="mb-4 leading-relaxed">    def __init__(self, d_model, max_length=5000):</p><p class="mb-4 leading-relaxed">        super().__init__()</p><br/><p class="mb-4 leading-relaxed">        pe = torch.zeros(max_length, d_model)</p><p class="mb-4 leading-relaxed">        position = torch.arange(0, max_length).unsqueeze(1).float()</p><br/><p class="mb-4 leading-relaxed">        div_term = torch.exp(torch.arange(0, d_model, 2).float() *</p><p class="mb-4 leading-relaxed">                           -(math.log(10000.0) / d_model))</p><br/><p class="mb-4 leading-relaxed">        pe[:, 0::2] = torch.sin(position * div_term)</p><p class="mb-4 leading-relaxed">        pe[:, 1::2] = torch.cos(position * div_term)</p><br/><p class="mb-4 leading-relaxed">        self.register_buffer(&#x27;pe&#x27;, pe.unsqueeze(0))</p><br/><p class="mb-4 leading-relaxed">    def forward(self, x):</p><p class="mb-4 leading-relaxed">        return x + self.pe[:, :x.size(1)]</p><br/><h2 class="text-2xl font-bold mb-4 mt-6 text-primary test-font-courier">Optimization Techniques</h2><br/><h3 class="text-xl font-semibold mb-3 mt-4 text-primary test-font-courier">Memory Efficiency</h3><br/><p class="mb-4 leading-relaxed">1. **Gradient Checkpointing**: Trade compute for memory by recomputing activations during backward pass</p><p class="mb-4 leading-relaxed">2. **Mixed Precision Training**: Use FP16 for forward pass, FP32 for gradients</p><p class="mb-4 leading-relaxed">3. **Model Parallelism**: Split large models across multiple GPUs</p><br/><h3 class="text-xl font-semibold mb-3 mt-4 text-primary test-font-courier">Compute Efficiency</h3><br/><p class="mb-4 leading-relaxed">1. **Flash Attention**: More memory-efficient attention implementation</p><p class="mb-4 leading-relaxed">2. **Sparse Attention**: Only compute attention for relevant token pairs</p><p class="mb-4 leading-relaxed">3. **Quantization**: Reduce model size with minimal accuracy loss</p><br/><h2 class="text-2xl font-bold mb-4 mt-6 text-primary test-font-courier">Performance Benchmarks</h2><br/><p class="mb-4 leading-relaxed">We tested our implementation against standard transformer models:</p><br/><br/><h2 class="text-2xl font-bold mb-4 mt-6 text-primary test-font-courier">Conclusion</h2><br/><p class="mb-4 leading-relaxed">Building efficient transformers requires careful attention to both architecture and implementation details. The optimizations discussed here can significantly improve both memory usage and computational efficiency while maintaining model performance.</p><br/><h2 class="text-2xl font-bold mb-4 mt-6 text-primary test-font-courier">References</h2><br/><li class="mb-2 ml-4">Vaswani, A., et al. (2017). &quot;Attention is all you need.&quot; NIPS.</li><li class="mb-2 ml-4">Dao, T., et al. (2022). &quot;FlashAttention: Fast and memory-efficient exact attention with IO-awareness.&quot; ICML.</li><li class="mb-2 ml-4">Brown, T., et al. (2020). &quot;Language models are few-shot learners.&quot; NeurIPS.</li><br/></div></div></div></div></main></div><script src="/_next/static/chunks/webpack-52aa0ba6fea5041f.js" crossorigin="" async=""></script><script>(self.__next_f=self.__next_f||[]).push([0]);self.__next_f.push([2,null])</script><script>self.__next_f.push([1,"1:HL[\"/_next/static/media/e4af272ccee01ff0-s.p.woff2\",\"font\",{\"crossOrigin\":\"\",\"type\":\"font/woff2\"}]\n2:HL[\"/_next/static/css/5ce533f03faec87d.css\",\"style\",{\"crossOrigin\":\"\"}]\n0:\"$L3\"\n"])</script><script>self.__next_f.push([1,"4:I[3728,[],\"\"]\n6:I[9928,[],\"\"]\n7:I[6954,[],\"\"]\n8:I[7264,[],\"\"]\na:I[8326,[\"326\",\"static/chunks/326-f3014610e927a105.js\",\"308\",\"static/chunks/app/blog/%5Bslug%5D/page-59ec5e1211dbfc7e.js\"],\"\"]\n"])</script><script>self.__next_f.push([1,"3:[[[\"$\",\"link\",\"0\",{\"rel\":\"stylesheet\",\"href\":\"/_next/static/css/5ce533f03faec87d.css\",\"precedence\":\"next\",\"crossOrigin\":\"\"}]],[\"$\",\"$L4\",null,{\"buildId\":\"h7mXkjEwxRjzbG3e8cpKQ\",\"assetPrefix\":\"\",\"initialCanonicalUrl\":\"/blog/efficient-transformers-from-scratch/\",\"initialTree\":[\"\",{\"children\":[\"blog\",{\"children\":[[\"slug\",\"efficient-transformers-from-scratch\",\"d\"],{\"children\":[\"__PAGE__?{\\\"slug\\\":\\\"efficient-transformers-from-scratch\\\"}\",{}]}]}]},\"$undefined\",\"$undefined\",true],\"initialHead\":[false,\"$L5\"],\"globalErrorComponent\":\"$6\",\"children\":[null,[\"$\",\"html\",null,{\"lang\":\"en\",\"className\":\"dark\",\"children\":[\"$\",\"body\",null,{\"className\":\"__className_e8ce0c\",\"children\":[\"$\",\"div\",null,{\"className\":\"min-h-screen bg-background text-foreground\",\"children\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":[[\"$\",\"title\",null,{\"children\":\"404: This page could not be found.\"}],[\"$\",\"div\",null,{\"style\":{\"fontFamily\":\"system-ui,\\\"Segoe UI\\\",Roboto,Helvetica,Arial,sans-serif,\\\"Apple Color Emoji\\\",\\\"Segoe UI Emoji\\\"\",\"height\":\"100vh\",\"textAlign\":\"center\",\"display\":\"flex\",\"flexDirection\":\"column\",\"alignItems\":\"center\",\"justifyContent\":\"center\"},\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"style\",null,{\"dangerouslySetInnerHTML\":{\"__html\":\"body{color:#000;background:#fff;margin:0}.next-error-h1{border-right:1px solid rgba(0,0,0,.3)}@media (prefers-color-scheme:dark){body{color:#fff;background:#000}.next-error-h1{border-right:1px solid rgba(255,255,255,.3)}}\"}}],[\"$\",\"h1\",null,{\"className\":\"next-error-h1\",\"style\":{\"display\":\"inline-block\",\"margin\":\"0 20px 0 0\",\"padding\":\"0 23px 0 0\",\"fontSize\":24,\"fontWeight\":500,\"verticalAlign\":\"top\",\"lineHeight\":\"49px\"},\"children\":\"404\"}],[\"$\",\"div\",null,{\"style\":{\"display\":\"inline-block\"},\"children\":[\"$\",\"h2\",null,{\"style\":{\"fontSize\":14,\"fontWeight\":400,\"lineHeight\":\"49px\",\"margin\":0},\"children\":\"This page could not be found.\"}]}]]}]}]],\"notFoundStyles\":[],\"childProp\":{\"current\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$\",\"$L7\",null,{\"parallelRouterKey\":\"children\",\"segmentPath\":[\"children\",\"blog\",\"children\",[\"slug\",\"efficient-transformers-from-scratch\",\"d\"],\"children\"],\"loading\":\"$undefined\",\"loadingStyles\":\"$undefined\",\"loadingScripts\":\"$undefined\",\"hasLoading\":false,\"error\":\"$undefined\",\"errorStyles\":\"$undefined\",\"errorScripts\":\"$undefined\",\"template\":[\"$\",\"$L8\",null,{}],\"templateStyles\":\"$undefined\",\"templateScripts\":\"$undefined\",\"notFound\":\"$undefined\",\"notFoundStyles\":\"$undefined\",\"childProp\":{\"current\":[\"$L9\",[\"$\",\"main\",null,{\"className\":\"min-h-screen bg-background text-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"bg-background border-b border-border\",\"children\":[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-8\",\"children\":[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 mb-6\",\"children\":[\"$\",\"$La\",null,{\"href\":\"/blog\",\"children\":[\"$\",\"button\",null,{\"className\":\"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4 mr-2\",\"children\":[[\"$\",\"path\",\"1l729n\",{\"d\":\"m12 19-7-7 7-7\"}],[\"$\",\"path\",\"x3x0zl\",{\"d\":\"M19 12H5\"}],\"$undefined\"]}],\"Back to Blog\"]}]}]}]}]}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 py-12\",\"children\":[\"$\",\"div\",null,{\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center gap-2 mb-4\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 text-foreground text-xs font-medium\",\"children\":\"TUTORIAL\"}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-4 text-sm text-muted-foreground\",\"children\":[[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4\",\"children\":[[\"$\",\"rect\",\"eu3xkr\",{\"width\":\"18\",\"height\":\"18\",\"x\":\"3\",\"y\":\"4\",\"rx\":\"2\",\"ry\":\"2\"}],[\"$\",\"line\",\"m3sa8f\",{\"x1\":\"16\",\"x2\":\"16\",\"y1\":\"2\",\"y2\":\"6\"}],[\"$\",\"line\",\"18kwsl\",{\"x1\":\"8\",\"x2\":\"8\",\"y1\":\"2\",\"y2\":\"6\"}],[\"$\",\"line\",\"xt86sb\",{\"x1\":\"3\",\"x2\":\"21\",\"y1\":\"10\",\"y2\":\"10\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"March 15, 2024\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center space-x-1\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4\",\"children\":[[\"$\",\"circle\",\"1mglay\",{\"cx\":\"12\",\"cy\":\"12\",\"r\":\"10\"}],[\"$\",\"polyline\",\"68esgv\",{\"points\":\"12 6 12 12 16 14\"}],\"$undefined\"]}],[\"$\",\"span\",null,{\"children\":\"12 min read\"}]]}]]}]]}],[\"$\",\"h1\",null,{\"className\":\"text-4xl md:text-5xl font-bold mb-6 test-font-courier text-primary leading-tight\",\"children\":\"Building Efficient Transformers from Scratch\"}],[\"$\",\"p\",null,{\"className\":\"text-xl text-muted-foreground mb-8 test-font-mono leading-relaxed\",\"children\":\"A comprehensive guide to implementing transformer architectures with optimizations for memory and compute efficiency. Includes PyTorch code examples and performance benchmarks.\"}],[\"$\",\"div\",null,{\"className\":\"flex flex-wrap gap-2 mb-8\",\"children\":[[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"PyTorch\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"Transformers\"}],[\"$\",\"div\",null,{\"className\":\"inline-flex items-center rounded-md border px-2.5 py-0.5 font-semibold transition-colors focus:outline-none focus:ring-2 focus:ring-ring focus:ring-offset-2 border-transparent bg-secondary text-secondary-foreground hover:bg-secondary/80 text-sm\",\"children\":\"Tutorial\"}]]}],[\"$\",\"div\",null,{\"className\":\"flex items-center gap-4 pb-8 border-b border-border\",\"children\":[[\"$\",\"button\",null,{\"className\":\"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4 mr-2\",\"children\":[[\"$\",\"circle\",\"gq8acd\",{\"cx\":\"18\",\"cy\":\"5\",\"r\":\"3\"}],[\"$\",\"circle\",\"w7nqdw\",{\"cx\":\"6\",\"cy\":\"12\",\"r\":\"3\"}],[\"$\",\"circle\",\"1xt0gg\",{\"cx\":\"18\",\"cy\":\"19\",\"r\":\"3\"}],[\"$\",\"line\",\"47mynk\",{\"x1\":\"8.59\",\"x2\":\"15.42\",\"y1\":\"13.51\",\"y2\":\"17.49\"}],[\"$\",\"line\",\"1n3mei\",{\"x1\":\"15.41\",\"x2\":\"8.59\",\"y1\":\"6.51\",\"y2\":\"10.49\"}],\"$undefined\"]}],\"Share\"]}],[\"$\",\"button\",null,{\"className\":\"inline-flex items-center justify-center whitespace-nowrap text-sm font-medium ring-offset-background transition-colors focus-visible:outline-none focus-visible:ring-2 focus-visible:ring-ring focus-visible:ring-offset-2 disabled:pointer-events-none disabled:opacity-50 border border-input bg-background hover:bg-accent hover:text-accent-foreground h-9 rounded-md px-3 test-font-mono\",\"children\":[[\"$\",\"svg\",null,{\"xmlns\":\"http://www.w3.org/2000/svg\",\"width\":24,\"height\":24,\"viewBox\":\"0 0 24 24\",\"fill\":\"none\",\"stroke\":\"currentColor\",\"strokeWidth\":2,\"strokeLinecap\":\"round\",\"strokeLinejoin\":\"round\",\"className\":\"w-4 h-4 mr-2\",\"children\":[[\"$\",\"path\",\"vv98re\",{\"d\":\"M2 3h6a4 4 0 0 1 4 4v14a3 3 0 0 0-3-3H2z\"}],[\"$\",\"path\",\"1cyq3y\",{\"d\":\"M22 3h-6a4 4 0 0 0-4 4v14a3 3 0 0 1 3-3h7z\"}],\"$undefined\"]}],\"Save\"]}]]}]]}]}],[\"$\",\"div\",null,{\"className\":\"max-w-4xl mx-auto px-6 pb-20\",\"children\":[\"$\",\"div\",null,{\"className\":\"relative rounded-xl bg-card text-card-foreground border-0 shadow-none\",\"children\":[\"$\",\"div\",null,{\"className\":\"p-0\",\"children\":[\"$\",\"div\",null,{\"className\":\"prose prose-lg max-w-none test-font-mono\",\"style\":{\"color\":\"var(--foreground)\",\"lineHeight\":\"1.7\"},\"children\":[[\"$\",\"br\",\"0\",{}],[\"$\",\"h1\",\"1\",{\"className\":\"text-3xl font-bold mb-6 mt-8 text-primary test-font-courier\",\"children\":\"Building Efficient Transformers from Scratch\"}],[\"$\",\"br\",\"2\",{}],[\"$\",\"p\",\"3\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"Transformers have revolutionized the field of natural language processing and beyond. In this comprehensive tutorial, we'll build efficient transformer architectures from scratch, focusing on memory and compute optimizations.\"}],[\"$\",\"br\",\"4\",{}],[\"$\",\"h2\",\"5\",{\"className\":\"text-2xl font-bold mb-4 mt-6 text-primary test-font-courier\",\"children\":\"Introduction\"}],[\"$\",\"br\",\"6\",{}],[\"$\",\"p\",\"7\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"The transformer architecture, introduced in \\\"Attention Is All You Need\\\" (Vaswani et al., 2017), has become the foundation for most state-of-the-art models in NLP, computer vision, and other domains. Understanding how to implement and optimize transformers is crucial for any ML practitioner.\"}],[\"$\",\"br\",\"8\",{}],[\"$\",\"h2\",\"9\",{\"className\":\"text-2xl font-bold mb-4 mt-6 text-primary test-font-courier\",\"children\":\"Key Components\"}],[\"$\",\"br\",\"10\",{}],[\"$\",\"h3\",\"11\",{\"className\":\"text-xl font-semibold mb-3 mt-4 text-primary test-font-courier\",\"children\":\"1. Multi-Head Attention\"}],[\"$\",\"br\",\"12\",{}],[\"$\",\"p\",\"13\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"The core innovation of transformers is the multi-head attention mechanism. Here's a simplified implementation:\"}],[\"$\",\"br\",\"14\",{}],null,[\"$\",\"p\",\"16\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"import torch\"}],[\"$\",\"p\",\"17\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"import torch.nn as nn\"}],[\"$\",\"p\",\"18\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"import torch.nn.functional as F\"}],[\"$\",\"br\",\"19\",{}],[\"$\",\"p\",\"20\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"class MultiHeadAttention(nn.Module):\"}],[\"$\",\"p\",\"21\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"    def __init__(self, d_model, num_heads):\"}],[\"$\",\"p\",\"22\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        super().__init__()\"}],[\"$\",\"p\",\"23\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.d_model = d_model\"}],[\"$\",\"p\",\"24\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.num_heads = num_heads\"}],[\"$\",\"p\",\"25\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.d_k = d_model // num_heads\"}],[\"$\",\"br\",\"26\",{}],[\"$\",\"p\",\"27\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.W_q = nn.Linear(d_model, d_model)\"}],[\"$\",\"p\",\"28\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.W_k = nn.Linear(d_model, d_model)\"}],[\"$\",\"p\",\"29\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.W_v = nn.Linear(d_model, d_model)\"}],[\"$\",\"p\",\"30\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.W_o = nn.Linear(d_model, d_model)\"}],[\"$\",\"br\",\"31\",{}],[\"$\",\"p\",\"32\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"    def forward(self, query, key, value, mask=None):\"}],[\"$\",\"p\",\"33\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        batch_size = query.size(0)\"}],[\"$\",\"br\",\"34\",{}],[\"$\",\"p\",\"35\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        # Linear transformations\"}],[\"$\",\"p\",\"36\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        Q = self.W_q(query)\"}],[\"$\",\"p\",\"37\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        K = self.W_k(key)\"}],[\"$\",\"p\",\"38\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        V = self.W_v(value)\"}],[\"$\",\"br\",\"39\",{}],[\"$\",\"p\",\"40\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        # Reshape for multi-head attention\"}],[\"$\",\"p\",\"41\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        Q = Q.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\"}],[\"$\",\"p\",\"42\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        K = K.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\"}],[\"$\",\"p\",\"43\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        V = V.view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\"}],[\"$\",\"br\",\"44\",{}],[\"$\",\"p\",\"45\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        # Scaled dot-product attention\"}],[\"$\",\"p\",\"46\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5)\"}],[\"$\",\"br\",\"47\",{}],[\"$\",\"p\",\"48\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        if mask is not None:\"}],[\"$\",\"p\",\"49\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"            scores = scores.masked_fill(mask == 0, -1e9)\"}],[\"$\",\"br\",\"50\",{}],[\"$\",\"p\",\"51\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        attention_weights = F.softmax(scores, dim=-1)\"}],[\"$\",\"p\",\"52\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        context = torch.matmul(attention_weights, V)\"}],[\"$\",\"br\",\"53\",{}],[\"$\",\"p\",\"54\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        # Concatenate heads\"}],[\"$\",\"p\",\"55\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        context = context.transpose(1, 2).contiguous().view(\"}],[\"$\",\"p\",\"56\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"            batch_size, -1, self.d_model\"}],[\"$\",\"p\",\"57\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        )\"}],[\"$\",\"br\",\"58\",{}],[\"$\",\"p\",\"59\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        return self.W_o(context)\"}],null,[\"$\",\"br\",\"61\",{}],[\"$\",\"h3\",\"62\",{\"className\":\"text-xl font-semibold mb-3 mt-4 text-primary test-font-courier\",\"children\":\"2. Position Encoding\"}],[\"$\",\"br\",\"63\",{}],[\"$\",\"p\",\"64\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"Since transformers don't have inherent positional information, we need to add position encodings:\"}],[\"$\",\"br\",\"65\",{}],null,[\"$\",\"p\",\"67\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"class PositionalEncoding(nn.Module):\"}],[\"$\",\"p\",\"68\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"    def __init__(self, d_model, max_length=5000):\"}],[\"$\",\"p\",\"69\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        super().__init__()\"}],[\"$\",\"br\",\"70\",{}],[\"$\",\"p\",\"71\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        pe = torch.zeros(max_length, d_model)\"}],[\"$\",\"p\",\"72\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        position = torch.arange(0, max_length).unsqueeze(1).float()\"}],[\"$\",\"br\",\"73\",{}],[\"$\",\"p\",\"74\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        div_term = torch.exp(torch.arange(0, d_model, 2).float() *\"}],[\"$\",\"p\",\"75\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"                           -(math.log(10000.0) / d_model))\"}],[\"$\",\"br\",\"76\",{}],[\"$\",\"p\",\"77\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        pe[:, 0::2] = torch.sin(position * div_term)\"}],[\"$\",\"p\",\"78\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        pe[:, 1::2] = torch.cos(position * div_term)\"}],[\"$\",\"br\",\"79\",{}],[\"$\",\"p\",\"80\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        self.register_buffer('pe', pe.unsqueeze(0))\"}],[\"$\",\"br\",\"81\",{}],[\"$\",\"p\",\"82\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"    def forward(self, x):\"}],[\"$\",\"p\",\"83\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"        return x + self.pe[:, :x.size(1)]\"}],null,[\"$\",\"br\",\"85\",{}],[\"$\",\"h2\",\"86\",{\"className\":\"text-2xl font-bold mb-4 mt-6 text-primary test-font-courier\",\"children\":\"Optimization Techniques\"}],[\"$\",\"br\",\"87\",{}],[\"$\",\"h3\",\"88\",{\"className\":\"text-xl font-semibold mb-3 mt-4 text-primary test-font-courier\",\"children\":\"Memory Efficiency\"}],[\"$\",\"br\",\"89\",{}],[\"$\",\"p\",\"90\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"1. **Gradient Checkpointing**: Trade compute for memory by recomputing activations during backward pass\"}],[\"$\",\"p\",\"91\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"2. **Mixed Precision Training**: Use FP16 for forward pass, FP32 for gradients\"}],[\"$\",\"p\",\"92\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"3. **Model Parallelism**: Split large models across multiple GPUs\"}],[\"$\",\"br\",\"93\",{}],[\"$\",\"h3\",\"94\",{\"className\":\"text-xl font-semibold mb-3 mt-4 text-primary test-font-courier\",\"children\":\"Compute Efficiency\"}],[\"$\",\"br\",\"95\",{}],[\"$\",\"p\",\"96\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"1. **Flash Attention**: More memory-efficient attention implementation\"}],[\"$\",\"p\",\"97\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"2. **Sparse Attention**: Only compute attention for relevant token pairs\"}],[\"$\",\"p\",\"98\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"3. **Quantization**: Reduce model size with minimal accuracy loss\"}],[\"$\",\"br\",\"99\",{}],[\"$\",\"h2\",\"100\",{\"className\":\"text-2xl font-bold mb-4 mt-6 text-primary test-font-courier\",\"children\":\"Performance Benchmarks\"}],[\"$\",\"br\",\"101\",{}],[\"$\",\"p\",\"102\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"We tested our implementation against standard transformer models:\"}],[\"$\",\"br\",\"103\",{}],null,null,null,null,null,[\"$\",\"br\",\"109\",{}],[\"$\",\"h2\",\"110\",{\"className\":\"text-2xl font-bold mb-4 mt-6 text-primary test-font-courier\",\"children\":\"Conclusion\"}],[\"$\",\"br\",\"111\",{}],[\"$\",\"p\",\"112\",{\"className\":\"mb-4 leading-relaxed\",\"children\":\"Building efficient transformers requires careful attention to both architecture and implementation details. The optimizations discussed here can significantly improve both memory usage and computational efficiency while maintaining model performance.\"}],[\"$\",\"br\",\"113\",{}],[\"$\",\"h2\",\"114\",{\"className\":\"text-2xl font-bold mb-4 mt-6 text-primary test-font-courier\",\"children\":\"References\"}],[\"$\",\"br\",\"115\",{}],[\"$\",\"li\",\"116\",{\"className\":\"mb-2 ml-4\",\"children\":\"Vaswani, A., et al. (2017). \\\"Attention is all you need.\\\" NIPS.\"}],[\"$\",\"li\",\"117\",{\"className\":\"mb-2 ml-4\",\"children\":\"Dao, T., et al. (2022). \\\"FlashAttention: Fast and memory-efficient exact attention with IO-awareness.\\\" ICML.\"}],[\"$\",\"li\",\"118\",{\"className\":\"mb-2 ml-4\",\"children\":\"Brown, T., et al. (2020). \\\"Language models are few-shot learners.\\\" NeurIPS.\"}],[\"$\",\"br\",\"119\",{}]]}]}]}]}]]}],null],\"segment\":\"__PAGE__?{\\\"slug\\\":\\\"efficient-transformers-from-scratch\\\"}\"},\"styles\":null}],\"segment\":[\"slug\",\"efficient-transformers-from-scratch\",\"d\"]},\"styles\":null}],\"segment\":\"blog\"},\"styles\":null}]}]}]}],null]}]]\n"])</script><script>self.__next_f.push([1,"5:[[\"$\",\"meta\",\"0\",{\"name\":\"viewport\",\"content\":\"width=device-width, initial-scale=1\"}],[\"$\",\"meta\",\"1\",{\"charSet\":\"utf-8\"}],[\"$\",\"title\",\"2\",{\"children\":\"Sai Amruth Balusu\"}],[\"$\",\"meta\",\"3\",{\"name\":\"description\",\"content\":\"Personal website of Sai Amruth Balusu, Machine Learning Researcher specializing in transformers, computer vision, and distributed systems.\"}],[\"$\",\"meta\",\"4\",{\"name\":\"author\",\"content\":\"Sai Amruth Balusu\"}],[\"$\",\"meta\",\"5\",{\"name\":\"keywords\",\"content\":\"machine learning,AI research,deep learning,transformers,computer vision\"}],[\"$\",\"meta\",\"6\",{\"property\":\"og:title\",\"content\":\"Sai Amruth Balusu - ML Researcher\"}],[\"$\",\"meta\",\"7\",{\"property\":\"og:description\",\"content\":\"Personal website showcasing research, publications, and projects in machine learning.\"}],[\"$\",\"meta\",\"8\",{\"property\":\"og:url\",\"content\":\"https://amruv.github.io/\"}],[\"$\",\"meta\",\"9\",{\"property\":\"og:site_name\",\"content\":\"Sai Amruth Balusu\"}],[\"$\",\"meta\",\"10\",{\"property\":\"og:image\",\"content\":\"https://amruv.github.io/og-image.png\"}],[\"$\",\"meta\",\"11\",{\"property\":\"og:image:width\",\"content\":\"1200\"}],[\"$\",\"meta\",\"12\",{\"property\":\"og:image:height\",\"content\":\"630\"}],[\"$\",\"meta\",\"13\",{\"property\":\"og:image:alt\",\"content\":\"Sai Amruth Balusu - ML Researcher\"}],[\"$\",\"meta\",\"14\",{\"property\":\"og:type\",\"content\":\"website\"}],[\"$\",\"meta\",\"15\",{\"name\":\"twitter:card\",\"content\":\"summary_large_image\"}],[\"$\",\"meta\",\"16\",{\"name\":\"twitter:title\",\"content\":\"Sai Amruth Balusu - ML Researcher\"}],[\"$\",\"meta\",\"17\",{\"name\":\"twitter:description\",\"content\":\"Personal website showcasing research, publications, and projects in machine learning.\"}],[\"$\",\"meta\",\"18\",{\"name\":\"twitter:image\",\"content\":\"https://amruv.github.io/og-image.png\"}],[\"$\",\"meta\",\"19\",{\"name\":\"next-size-adjust\"}]]\n"])</script><script>self.__next_f.push([1,"9:null\n"])</script><script>self.__next_f.push([1,""])</script></body></html>