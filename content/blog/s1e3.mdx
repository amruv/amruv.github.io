---
title: "How tf do LLMs work? - Multi-Head Attention"
description: "Understanding the attention mechanism - the engine that powers transformers"
date: "2025-09-28"
readTime: "8 min read"
tags: ["transformers", "llm", "attention", "multi-head-attention", "self-attention"]
series: "How tf do LLMs work?"
category: 'EXPLAINER'
part: 3
gradient: "from-pink-500/20 to-orange-500/20"
---

# The Engine of the Transformer Model - Multi-Head Attention

Now that we've caught up with all the prerequisites of the transformer model, let's now delve into the meat and potatoes of this research blog: the components the self-attention mechanism comprises of and how each component fits into the larger ecosystem and contributes to the working of the building block of modern day LLMs that is Multi-Head Attention.

## The general idea of Attention

Okay, consider the sentence 'As he saw money on the street, he went to pick it up'. For a machine, which works in deterministic logic, the meaning of 'it' here is ambiguous. It could refer to either of the nouns - 'money' or 'street'. However, a human brain can easily understand what the 'it' means because it understands what to focus on. This selective focus is what is termed as **'attention'**.

In mathematical form, **attention** is considered the weighted sum of **Values (V)** - where the weights are calculated as the relevance between a **Query (Q)** and a set of **Keys (K)**.

**Query (Q)** is essentially the word that is asking its relevance in the meaning of a sentence. A query vector is calculated for each token in a given input which is then used to determine its relationship with other tokens in the input. In the case above, the Query is the word 'it'.

**Keys (K)** is the list of all the tokens present in the sentence (or context, in real-life use cases), including the current query token. Each token has a corresponding key vector and they serve as reference vectors to which a given query vector is compared against. Here, all the words in the sentence constitute the keys of the input.

**Values (V)** are the vectors of each token that hold the semantic meaning of a given word in an input. (This out of all the 3 components is, in my opinion, the most relied on good training of a model, which can determine how accurate the value vectors are calculated for any given attention calculation).

These vectors are generated by multiplying the input embedding 'x' by three separate weight matrices (W<sub>Q</sub>, W<sub>K</sub>, W<sub>V</sub>) which are learned during training.

```
Q = x · W_Q
K = x · W_K
V = x · W_V
```


### The Scaled dot-product Attention (used in [1]) is shown as:

![Scaled Dot-Product Attention Formulae](/Images/attention_formula.png)

Basically:

**Score (QK<sup>T</sup>)** - Compare query to all keys (dot product), producing a **score matrix**.

![Attention Score](/Images/att_score.png)

**Scale (/√d<sub>k</sub>)** - Scaling down to prevent exploding values, where d<sub>k</sub> is the dimension of the key vectors.

![Attention Scaled](/Images/att_scaled.png)

**Weights (softmax)** - to turn scores into probabilities, called **attention weights**.

![Attention Weights Formula](/Images/att_weights.png)

**Output (Weighted Sum of V)** - Taking the weighted sum of the values. This final output vector is a rich representation, filled with information from all other tokens it paid attention to. (Essentially, attention weights × V)

![Attention Weights in terms of V - final formula](/Images/att_v.png)

## Multi-Head Attention

A single calculation of self-attention is immense but might help in understanding only one type of context of the given input. Observing this limitation, [1] proposed running the self-attention mechanism multiple times in parallel each with different weight matrices (W<sub>Q</sub>, W<sub>K</sub>, W<sub>V</sub>) to understand the different types of relationships in any input sentence. Essentially, each self-attention mechanism focuses on a different aspect of the input and is called a **'head'**, hence the nomenclature **'multi-head attention'**.

![MUlti-Head Attention Diagram](/Images/mh_attention.png)

### For all the math nerds:

For each head, independent matrices Q, K and V are calculated using the different weight matrices (W<sub>Q</sub>, W<sub>K</sub>, W<sub>V</sub>) where the variable *i* can run from 1 to *h* which is the number of heads. *(All these weight matrices are learned during training, btw)*

![Independentr weights input for each Head](/Images/mha_diff_weights.png)

Then for each head, the attention is calculated independently and in parallel to the other heads.

![Attention in Each Head](/Images/att_for_mha.png)

Finally, the outputs from all the attention heads are then concatenated and passed through a final weight matrix, W<sub>O</sub> to arrive at the final output:

![Multi-Head Attention Formulae](/Images/mha_formula.png)

## Why Multi-Head Attention is Crucial for Context Understanding

Using multiple processes of attention independently in parallel allows us to identify different types of semantics in the same input sentence. For instance, one head may focus on the understanding of how the verb relates to the subject of a sentence. Another head may reveal how the preposition ties the sentence together. Putting all these understandings together, the transformer arrives at a context rich meaning of any given sentence.

---

*This is part 3 of the series 'How tf do LLMs work?'. Stay tuned for the next part where we explore feed-forward networks and layer normalization!*

## References

[1] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.

[2] "A Deep Dive into the Self-Attention Mechanism of Transformers" blog post by Shreya Srivastava - *Image Credits*