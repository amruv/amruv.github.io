---
title: "How tf do LLMs work? - Part 2"
description: "It all starts with a bit of attention-seeking, I suppose."
date: "2025-09-07"
readTime: "12 min read"
tags: ["transformers", "llm", "attention", "machine-learning"]
category: "TUTORIAL"
gradient: "from-blue-500/20 to-purple-500/20"
---

# [Part 2] How tf do LLMs work?

This is a placeholder while we wire up MDX correctly.
